\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[spanish, es-noshorthands]{babel}
\usepackage{amssymb,amsmath,amsthm,amsfonts,mathtools}
\usepackage{tcolorbox}
\decimalpoint
\setlength{\parskip}{0.4em}
\setlength{\parindent}{0cm}
\usepackage[a4paper, width=160mm, top=25mm, bottom = 25mm]{geometry}
\usepackage{fancyhdr}

\pagestyle{fancy}
\fancyhf{}
\lhead{Procesos a tiempo continuo}
\cfoot{\thepage}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}
\setlength{\headheight}{15pt}

\title{
    Procesos estocásticos I \\
    \large Tarea: Procesos a tiempo continuo
}
\author{José Alberto Márquez Luján, \texttt{187917}}
\date{Verano 2022}

\newcommand{\E}{\mathbb{E}}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}

\begin{document}
\maketitle

% Pregunta 1
%------------------------------------------------------------
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!5!white, arc=0mm, boxrule=0pt]
\textbf{1.} Demuestre que un proceso de Poisson $N(\cdot)$ tiene trayectorias continuas no decrecientes, es decir, 
\[ P[N(t) \geq N(s)] = 1 \quad \forall t > s \geq 0. \]
\end{tcolorbox}

\textit{Solución}. 
\begin{align*}
    P(N(t) \geq N(s)) &= P(N(t) - N(s) \geq 0) \\
    &= \sum_{k=0}^{\infty} P(N(t) - N(s) = k) \\
    &= \sum_{k=0}^{\infty} P(N(s+t-s) - N(s) = k) \\
    &= \sum_{k=0}^{\infty} P(N(s+h) - N(s) = k).
\end{align*}

Como estamos hablando de incrementos estacionarios con $h=t-s$, y sabemos que se distribuyen como una variable aleatoria Poisson, entonces
    
\begin{align*}
    P(N(t) \geq N(s)) &= P(N(t) - N(s) \geq 0) \\
    &= \sum_{k=0}^{\infty} \frac{e^{-\lambda h}(\lambda h)^k}{k!} \\
    &= \sum_{k=0}^{\infty} \frac{e^{-\lambda (t-s)}[\lambda (t-s)]^k}{k!} \\
    &= 1.
\end{align*}

% Pregunta 2
%------------------------------------------------------------
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!5!white, arc=0mm, boxrule=0pt]
\textbf{2.} Demuestre:
\begin{itemize}
    \item[a)] Si $X(\cdot) = \{ X(t), t \geq 0 \}$ es un PE con incrementos independientes, entonces el PE $Y(\cdot)$ definido por $Y(t) := X(t)-X(0)$, para $t\geq0$, tiene incrementos independientes y $Y(0) = 0$.
\end{itemize}
\end{tcolorbox}

\textit{Solución}.
\begin{align*}
    Y(t_1) - Y(t_0) &= [X(t_1) - X(0)] - [X(t_0) - X(0)] \\
    &= X(t_1) - X(t_0) \\
    &\vdotswithin{=} \notag \\
    Y(t_n) - Y(t_{n-1}) &= [X(t_n) - X(0)] - [X(t_{n-1}) - X(0)] \\
    &= X(t_n) - X(t_{n-1})
\end{align*}

Por hipótesis, los incrementos de $X(\cdot)$ son independientes, por lo que los incrementos de $Y(\cdot)$ también lo son. Ahora bien, para $t=0$,
\[ Y(0) := X(0)-X(0) = 0. \]

\begin{tcolorbox}[colback=blue!5!white, colframe=blue!5!white, arc=0mm, boxrule=0pt]
\begin{itemize}
    \item[b)] Recíprocamente, sea $Y(\cdot) = \{ Y(t), t \geq 0 \}$ un PE con incrementos independientes y $Y(0)=0$, y sea $X(0)$ una v.a. independiente de $Y(\cdot)$. Entonces el PE $X(\cdot)$ definido por $X(t):=Y(t)+X(0)$ para todo $t\geq 0$ tiene incrementos independietes y estado inicial $X(0)$.
\end{itemize}
\end{tcolorbox}

\textit{Solución}. Falta.

% Pregunta 7
%------------------------------------------------------------
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!5!white, arc=0mm, boxrule=0pt]
\textbf{7.} Sea $N(\cdot)$ un proceso de Poisson, y sea $X_0$ una v.a. independiente de $N(\cdot)$ y con distribución $P(X_0=1) = P(X_0=-1) = 1/2$. En este caso, el PE $X(\cdot)$ definido por
\[ X(t) := X_0 (-1)^{N(t)} \quad t\geq0 \]
se dice que es una señal telegráfica aleatoria. Calcule la media y la función de covarianza de $X(\cdot)$.
\end{tcolorbox}

\textit{Solución.}
\begin{align*}
    \E[X] &= \E[X_0(-1)^{N(t)}] \\
    &\overset{\mathrm{ind}}{=} \E[X_0]\E[(-1)^{N(t)}].
\end{align*}

Ahora bien,
\[ \E[X_0] = (1)\left(\frac{1}{2}\right) + (-1)\left(\frac{1}{2}\right) = 0. \]
Por lo que $\E[X] = 0$.

Para la función de covarianza:
\begin{align*}
    K_X(s,t) &= \E[X(s)X(t)] - \E[X(s)] \E[X(t)] \\
    &= \E[X(s)X(t)] \\
    &= \E[(X_0(-1)^{N(s)})(X_0(-1)^{N(t)})] \\
    &= \E[X_0^2(-1)^{N(s)+N(t)}] \\
    &\overset{\mathrm{ind}}{=} \E[X_0^2]\E[(-1)^{N(s)+N(t)}].
\end{align*}

Calculamos el segundo momento:
\[ \E[X_0^2] = (1)^2\left(\frac{1}{2}\right) + (-1)^2\left(\frac{1}{2}\right) = 1. \]

Ahora,
\begin{align*}
    \E[(-1)^{N(s)+N(t)}] &= \E[(-1)^{N(s)+N(t)-N(s)+N(s)}] \\
    &= \E[(-1)^{N(t)-N(s)} (-1)^{2N(s)}] \\
    &= \E[(-1)^{N(t)-N(s)}],
\end{align*}
pues $2N(s)$ es par, por lo que $(-1)^{2N(s)} = 1$. Además, como $N(\cdot)$ es un proceso de Poisson,
\begin{align*}
    \E[(-1)^{N(s)+N(t)}] &= \sum_{x=0}^{\infty} \frac{[-\lambda(t-s)]^x e^{-\lambda(t-s)}}{x!} \\
    &= e^{-\lambda(t-s)} \sum_{x=0}^{\infty} \frac{[-\lambda(t-s)]^x}{x!} \\
    &= e^{-2\lambda(t-s)}.
\end{align*}

Finalmente,
\[ K_X(s,t) = e^{-2\lambda(t-s)}.\]



% Pregunta 8
%------------------------------------------------------------
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!5!white, arc=0mm, boxrule=0pt]
\textbf{8.} Calcule $\E[N(s)N(s+t)]$ para $s,t\geq 0$, donde $N(\cdot)$ es un proceso de Poisson con parámetro $\lambda$.
\end{tcolorbox}

\textit{Solución.} Sea $s < t$. 

\begin{align*}
    \E[N(s)N(s+t)] &= \E\{ N(s)[N(s+t)-N(s)+N(s)] \} \\
    &= \E\{ N(s)[N(s+t)-N(s)] \} + \E[N^2(s)] \\
    &= \E\{ [N(s)-N(0)][N(s+t)-N(s)] \} + \E[N^2(s)]
\end{align*}

Como son intervalos que no se traslapan,
\begin{align*}
    \E[N(s)N(s+t)] &= \E[N(s)-N(0)]\E[N(s+t)-N(s)] + \E[N^2(s)] \\
    &= \lambda s \lambda(s+t-s) + \E[N^2(s)] \\
    &= \lambda s \lambda t + (\textrm{Var}(N(s))+ \E^2[N(s)]) \\
    &= \lambda s \lambda t + (\lambda s + \lambda ^2 s^2) \\
    &= \lambda s (1 + \lambda t + \lambda s).
\end{align*}

El caso en el que $t < s$ es análogo.

Generalizando, 
\[\E[N(s)N(s+t)] = \lambda  \min\{s,t\}(1 + \lambda t + \lambda s). \]

% Pregunta 9
%------------------------------------------------------------
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!5!white, arc=0mm, boxrule=0pt]
\textbf{9.} Demuestre que si $N_1(\cdot)$ y $N_2(\cdot)$ son procesos de Poisson independientes con parámentros $\lambda_1$ y $\lambda_2$, entonces su suma $N_1(t) + N_2(t)$, $t\geq 0$, es un proceso de Poisson con parámetro $\lambda_1 + \lambda_2$. (Observación: la diferencia $N_1(t) - N_2(t)$ no es un proceso de Poisson).
\end{tcolorbox}

\textit{Solución.} Sea $Y(t) = N_1(t) + N_2(t)$. Primero vemos que $Y(\cdot)$ tenga incrementos independientes.

\begin{align*}
    Y(t_1) - Y(t_0) &= (N_1(t_1) + N_2(t_1)) - (N_1(t_0) + N_2(t_0)) \\
    &= (N_1(t_1) - N_1(t_0)) + (N_2(t_1) - N_2(t_0)) \\
    &\vdotswithin{=} \notag \\
    Y(t_n) - Y(t_{n-1}) &= (N_1(t_n) + N_2(t_n)) - (N_1(t_{n-1}) + N_2(t_{n-1})) \\
    &= (N_1(t_n) - N_1(t_{n-1})) + (N_2(t_n) - N_2(t_{n-1})).
\end{align*}

Como $N_1(\cdot)$ es un proceso de Poisson, entonces tiene incrementos independientes, por lo que $(N_1(t_1) - N_1(t_{0})) \bot (N_1(t_2) - N_1(t_{1}))$. Además, como $N_1(\cdot) \bot N_2(\cdot)$, entonces $(N_1(t_1) - N_1(t_{0})) \bot (N_2(t_1) - N_2(t_{0}))$; este razonamiento se utiliza para todos los renglones y así tenemos que el proceso tiene incrementos independientes.

Ahora revisamos que el proceso tenga incrementos estacionarios. Sean $t,s \geq 0$ y $h>0$.
\begin{align*}
    Y(t+h) - Y(t) &= (N_1(t+h)+N_2(t+h)) - (N_1(t)-N_2(t)) \\
    &= N_1(t+h)-N_1(t) + N_2(t+h)-N_2(t),
\end{align*}
y sabemos que $N_1(t+h)-N_1(t) \sim \textrm{Poisson}(\lambda_1 h)$ y $N_2(t+h)-N_2(t)\sim \textrm{Poisson}(\lambda_2 h)$. Así pues, como $N_1 \bot N_2$, $Y(t+h) - Y(t) \sim \textrm{Poisson}((\lambda_1 + \lambda_2) h)$. Análogamente, $Y(s+h) - Y(s) \sim \textrm{Poisson}((\lambda_1 + \lambda_2) h)$ y por lo tanto $Y(\cdot)$ tiene incrementos estacionarios.


% Pregunta 10
%------------------------------------------------------------
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!5!white, arc=0mm, boxrule=0pt]
\textbf{10.} Sea $N(\cdot)$ un proceso de Poisson con intensidad media $\lambda$, y sea $T$ na v.a. positiva independiente de $N(\cdot)$. Calcule $\E[N(T)]$ y $\textrm{Var}(N(T))$ en cada uno de los siguientes casos en los que $T$ tiene distribución:
\begin{itemize}
    \item[a)] exponencial con parámetro $\mu$
\end{itemize}
\end{tcolorbox}
Si $T \sim \textrm{Exp}(\mu)$, entonces $\E[T] = 1/\mu$ y $\textrm{Var}(T) = 1/\mu^2$. Así pues, 
\begin{align*}
    \E[N(T)] &\overset{\mathrm{c)}}{=} \lambda \E[T] \\
    &= \frac{\lambda}{\mu} \\
    \textrm{Var}(N(T)) &\overset{\mathrm{c)}}{=} \lambda \E[T] + \lambda^2 \textrm{Var}(T) \\
    &= \frac{\lambda}{\mu} + \frac{\lambda^2}{\mu^2}.
\end{align*}

\begin{tcolorbox}[colback=blue!5!white, colframe=blue!5!white, arc=0mm, boxrule=0pt]
\begin{itemize}
    \item[b)] uniforme en el intervalo $[a,b]$, con $0<a<b$
\end{itemize}
\end{tcolorbox}
Si $T \sim \textrm{Unif}(a,b)$, entonces $\E[T] = \frac{a+b}{2}$ y $\textrm{Var}(T) = \frac{(b-a)^2}{12}$. Así pues, 
\begin{align*}
    \E[N(T)] &\overset{\mathrm{c)}}{=} \lambda \E[T] \\
    &= \lambda \left( \frac{a+b}{2} \right) \\
    \textrm{Var}(N(T)) &\overset{\mathrm{c)}}{=} \lambda \E[T] + \lambda^2 \textrm{Var}(T) \\
    &= \lambda \left( \frac{a+b}{2} \right) + \frac{\lambda^2 (b-a)^2}{12} .
\end{align*}

\begin{tcolorbox}[colback=blue!5!white, colframe=blue!5!white, arc=0mm, boxrule=0pt]
\begin{itemize}
    \item[c)] continua arbitraria con densidad $f(t)$
\end{itemize}
\end{tcolorbox}
Vemos que, como $N(t) \sim \textrm{Poisson}(\lambda t)$,
\[ \E[N(T) \mid T = t] = \lambda t, \]
    por lo que 
    \[ \E[N(T) \mid T] = \lambda T, \]
y, por propiedades de la esperanza iterada, 
\begin{align*}
    \E[N(T)] &= \E [ \E[N(T) \mid T] ] \\
                &= \E [\lambda T] \\
                &= \lambda \E[T].
\end{align*}

Por otro lado, 
\begin{align*}
\E[N^2(T) \mid T = t] &= \textrm{Var}(N(t)) + \E^2[N(t)] \\    
&= \lambda t + \lambda^2 t^2
\end{align*}
Entonces
\[ \E[N^2(T) \mid T] = \lambda T + \lambda^2 T^2. \]

Por propiedades de la esperanza iterada, 
\begin{align*}
    \E[N^2(T)] &= \E [ \E[N^2(T) \mid T] ] \\
                &= \E [\lambda T + \lambda^2 T^2] \\
                &= \lambda \E[T] + \lambda^2 \E[T^2].
\end{align*}

Por lo tanto,
\begin{align*}
    \textrm{Var}(N(T)) &= \E[N^2(T)] - \E^2[N(T)] \\
    &= \lambda \E[T] + \lambda^2 \E[T^2] -  \lambda^2 \E^2[T] \\
    &= \lambda \E[T] + \lambda^2 \textrm{Var}(T).
\end{align*}

% Pregunta 11
%------------------------------------------------------------
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!5!white, arc=0mm, boxrule=0pt]
    \textbf{11.} Sea $N(\cdot)$ un proceso Poisson y sea 0 < s < t. Demuestre que la distribución condicional de $N(s)$, dado que $N(t) = n$, es la distribución $Bin(n,p)$ con $p = s/t$, es decir,
    \[ P[N(s) = k \mid N(t) = n] = \binom{n}{k}p^{k}(1-p)^{n-k} \textnormal{para k = 0,1,...,n}. \]
\end{tcolorbox} 

\textit{Solución}.
\begin{align*}
P(N(s) = k \mid N(t) = n) &= \frac{P(N(s)=k, N(t) = n)}{P(N(t) = n)}\\
&= \frac{P(N(s)-N(0) = k, N(t)-N(s) = n-k)}{P(N(t) = n)}\\
&\overset{\mathrm{ind}}{=} \frac{P[N(s)-N(0) = k] P[N(t)-N(s) = n-k]}{P(N(t) = n)} \\
&= \frac{\left(\frac{e^{-\lambda s} (\lambda s) ^{k}}{k!}\right) \left(\frac{e^{-\lambda (t-s)} (\lambda (t-s)) ^{n-k}}{(n-k)!}\right)}{\frac{e^{-\lambda t (\lambda t)^{n}}}{n!}} \\
&= \frac{e^{-\lambda s}(\lambda s)^{k} e^{-\lambda t} e^{\lambda s} (\lambda(t-s) )^{n-k} n! }{e^{-\lambda t}(\lambda t )^{n} k! (n-k)! }\\
&= \frac{n!}{k!(n-k)!} \frac{\lambda^{k} s^{k}\lambda^{n-k} (t-s)^{n-k} }{\lambda^{n}t^{n} }\\
&= \binom{n}{k} \frac{s^k(t-s)^{n-k}}{t^k t^{n-k}} \\
&= \binom{n}{k} \left(\frac{s}{t}\right)^{k} \left(\frac{t-s}{t}\right)^{n-k}.
\end{align*}

Por lo tanto, $ P(N(s) = k \mid N(t) = n) \sim \textnormal{Bin} \left(n, \frac{s}{t} \right)$.

% Pregunta 12
%------------------------------------------------------------
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!5!white, arc=0mm, boxrule=0pt]
    \textbf{12.} 
\end{tcolorbox} 

\textit{Solución}. ¿?

% Pregunta 13
%------------------------------------------------------------
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!5!white, arc=0mm, boxrule=0pt]
    \textbf{13.} El número de personas que pasan frente a un restaurante se comporta  como un proceso de Poisson con parámetro $\lambda = 1000$ personas por hora. Suponga que cada persona tiene probabilidad $p = 0.01$ de entrar al restaurante, y sea $Y$ el número de personas que entran en un periodo de 10 minutos. Calcule $\E[Y]$, $\Var(Y)$ y $P(Y \geq 2)$. Sugerencia: use el ejercicio anterior.

\end{tcolorbox} 

Sea $N$ el número de personas que pasa frente a un restaurante. Entonces, por hipótesis, $N(t) \sim Poisson(1000t)$, en donde $t = 10$ mins $ = 1/6$ de hora. Así pues, $N(1/6) \sim Poisson(500/3)$. 

Ahora, sea $X(\cdot) = \{ X(t), t \geq 0 \}$ el PE en el que $X(t)$ es el número de veces que una persona entra al restaurante en el intervalo $[0, t]$. Por el inciso anterior, $X(\cdot)$ es un proceso Poisson con parámetro $\lambda p = 10$; es decir, $ X(t) \sim Poisson(10t)$.

Nótese que podemos definir a $Y$ en términos de $X$, a saber:
\[ Y = X(t+1/6) - X(t) \]
Pero $X(t+1/6) - X(t) \sim Poisson(5/3)$, ya que $X$ tiene incrementos estacionarios. Así pues:
\begin{align*}
    \E[Y] &= \frac{5}{3}, \\
    \Var(Y) &= \frac{5}{3}, \quad \textrm{y}\\
    P(Y \geq 2) &= 1 - P(Y < 2) \\
    &= 1 - P(Y \leq 1) \\
    &= 1 - \left( \frac{e^{-5/3} \frac{5}{3}^{0} }{0!} + \frac{e^{-5/3} \frac{5}{3}^{1} }{1!}\right)\\
    &= 0.4963
\end{align*}

% Pregunta 14
%------------------------------------------------------------
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!5!white, arc=0mm, boxrule=0pt]
    \textbf{14.} El puente de Poisson. Sea $N(\cdot)$ un proceso de Poisson con parámetro $\lambda$, y sea $N_p(t) := N(t) - tN(1)$, para $0 \leq t \leq 1$, el llamado ``puente de Poisson''. Calcule $\E[N_p(t)]$ y la función de covarianza de $N_p (\cdot)$.
\end{tcolorbox} 

\textit{Solución}.
\begin{align*}
    \E[N_p(t)] &= \E[N(t) - tN(1)]\\
    &= \E[N(t)] - t\E[N(1)]\\
    &= \lambda t - t(\lambda) \\
    &= 0.\\
\end{align*}
\begin{align*}
    K_{Np}(s,t) &= \E[N_p(s) N_p(t)] - \E[N_p(s)]\E[N_p(t)]\\
    &= \E [N_p(s) N_p(t)] \\
    &= \E [(N(s)-sN(1))(N(t)-tN(1)]\\
    &= \E [(N(s)N(t)-sN(1)N(t)-t(N(1)N(s)+stN^{2}(1)] \\
    &= \E[N(s)N(t)] - s\E[N(1)N(t)] - t\E[N(1)N(s)] + st \E[N^2(1)].
\end{align*}    

Ahora bien:
\begin{align*}
\E [(N(s)N(t)] &= K_n(s,t) + \E[N(s)]\E[N(t)]\\
&= \lambda \min(s,t) + \lambda^{2}st\\
\E[N^{2}(1)] &= Var(N(1)) + E^{2}[N(1)]\\
&= \lambda + \lambda^{2}
\end{align*}

Entonces
\begin{align*}
    K_{N_p}(s,t) &= \lambda \min(s,t) + \lambda^{2}st - s(\lambda \min(1,t) + \lambda^{2}t) - t(\lambda \min(1,s) + \lambda^{2}s) + st(\lambda + \lambda^{2}) \\
    &= \lambda \min(s,t) - s\lambda\min(1,t) - t(\lambda \min(1,s)) \\
    &= \lambda \min(s,t) - s\lambda t - t\lambda s \\
    &= \lambda \min(s,t) - 2\lambda st,
\end{align*}
pues $t \leq 1$ y $s \leq 1$.

% Pregunta 15
%------------------------------------------------------------
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!5!white, arc=0mm, boxrule=0pt]
    \textbf{15.} El puente browniano. Repita el ejercicio anterior pero para el ``puente browniano'' $W_p(t) := W(t) - tW(1) $, con $0 \leq t \leq 1$, en donde $W(\cdot)$ es un proceso de Weiner o movimiento browniano.
\end{tcolorbox} 

\textit{Solución}.
\begin{align*}
    W(t) = W(t) - W(0) &= N(0, \sigma^{2}t) \textnormal{es un proceso de Weiner}\\
    \E[W_p(t)] &= \E[W(t) - tW(1)]\\
    &= \E[W(t)] - t\E[W(1)] \\
    &= 0 \textnormal{, pues es un proceso Weiner}\\ 
    K_{Wp}(s,t) &= \E[W_p(s) W_p(t)] - \E[W_p(s)]\E[W_p(t)]\\
    &= \E [W_p(s) W_p(t)] \\
    &= \E [(W(s)-sW(1))(W(t)-tW(1)]\\
    &= \E [(W(s)W(t)-sW(1)W(t)-t(W(1)W(s)+stW^{2}(1)]
\end{align*}

Cálculos auxiliares
\begin{align*}
\E [(W(s)W(t)] &= K_w(s,t) + \E[W(s)]\E[W(t)]\\
&= \sigma \textnormal{min(s,t)}\\
\E[stW^{2}(1)] &= Var(W(1)) + E^{2}[W(1)]\\
&= \sigma^{2}
\end{align*}
\begin{align*}
K_{wp}(s,t) =  & \sigma^{2} \textnormal{ min}(s,t) - \sigma^{2} s \textnormal{ min}(1,t) - \sigma^{2} s \textnormal{ min}(s,1)-\sigma^{2}st \\
=  & \sigma^{2} \textnormal{ min}(s,t) - \sigma^{2} st - \sigma^{2} st-\sigma^{2}st \\
=  & \sigma^{2} \textnormal{ min}(s,t) - \sigma^{2} st\\
=  & \sigma^{2}( \textnormal{ min}(s,t) - st)
\end{align*}

% Pregunta 16
%------------------------------------------------------------
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!5!white, arc=0mm, boxrule=0pt]
    \textbf{16.} Sea $X(t) = \{X(t), t \geq 0\}$ un PE que tiene incrementos independientes y función media $m(t) := \E[X(t)]$ finita para todo $t\geq 0$. Demuestre que si $0 \leq t_1 < \cdots t_n < t_{n+1}$, entonces
    \[ \E[X(t_{n+1}) \mid X(t_1), \ldots, X(t_n)] = X(t_n) + m(t_{n+1}) - m(t_n). \]
\end{tcolorbox} 

\noindent
\textit{Solución}. 
\begin{align*}
    \E[X(t_{n+1}) \mid X(t_1), \ldots, X(t_n)] &= \E[X(t_{n+1}) + X(t_n) - X(t_n) \mid X(t_1), \ldots, X(t_n)] \\
    \begin{split}
    &= \E[X(t_{n+1}) - X(t_n) \mid X(t_1), \ldots, X(t_n)] \\
        & \quad \quad + \E[X(t_n)\mid X(t_1), \ldots, X(t_n)]
    \end{split}  \\
    &= \E[X(t_{n+1}) - X(t_n)] + X(t_n) \\
    &= \E[X(t_{n+1})] - \E[X(t_n)] + X(t_n) \\
    &= X(t_n) + m(t_{n+1}) - m(t_n).
\end{align*}

% Pregunta 17
%------------------------------------------------------------
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!5!white, arc=0mm, boxrule=0pt]
    \textbf{Nota.} En lo sejercicios siguientes, $W(\cdot)$ es un proceso de Wiener con parámetro $\sigma^2 > 0$.
    
    \textbf{17.} Demuestre que $\E[(W(s)-W(a))(W(t)-W(a))] = \sigma^2 \min(s-a,t-a)$ para todo $s,t \geq a \geq 0$.
\end{tcolorbox} 

\noindent
\textit{Solución}.
\begin{align*}
    \E[(W(s)-W(a))(W(t)-W(a))] &= \E[W(s)W(t) - W(s)W(a)-W(t)W(a) + W^2(a)] \\
    \begin{split}
    &= \E[W(s)W(t)] - \E[W(s)W(a)] - \E[W(t)W(a)] \\ &\quad \quad + \E[W^2(a)].
    \end{split}
\end{align*}

Recordemos que $ K_W(s,t) = \Cov(W(s), W(t)) = \E[W(s)W(t)] - \E[W(s)]\E[W(t)]$, por lo que
\begin{align*}
    \E[W(s)W(t)] &= K_W(s,t) + \E[W(s)]\E[W(t)] \\
    &= \sigma^2 \min(s,t) + \E[W(s)]\E[W(t)] \\
    &= \sigma^2 \min(s,t),
\end{align*}
pues en clase vimos que $K_W(s,t) = \lambda \min(s,t)$. Análogamente, 
\[ \E[W(s)W(a)] = \sigma^2 \min(s,a) \quad \textrm{y} \quad \E[W(t)W(a)] = \sigma^2 \min(t,a). \]

También sabemos que 
\begin{align*}
    \E[W^2(a)] &= \Var(W(a)) + \E^2[W(a)] \\
    &= \sigma^2 a.
\end{align*}

Así pues, 
\begin{align*}
    \E[(W(s)-W(a))(W(t)-W(a))] 
    &= \sigma^2 \min(s,t) - \sigma^2 \min(s,a) - \sigma^2 \min(a,t) + \sigma^2a \\
    &= \sigma^2 \min(s,t) - \sigma^2 a - \sigma^2 a + \sigma^2 a \\
    &= \sigma^2 \min(s,t) - \sigma^2 a \\
    &= \sigma^2 [\min(s,t) - a] \\
    &= \sigma^2 \min(s-a, t-a).
\end{align*}

% Pregunta 18
%------------------------------------------------------------
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!5!white, arc=0mm, boxrule=0pt]  
    \textbf{18.} Demuestre que $W(1) + \cdots + W(n)$ tiene distribución $N(0, s_n)$, en donde
    \[ s_n := \sigma^2 \frac{n(n+1)(2n+1)}{6}. \]

    Sugerencia: use la independencia de los incrementos de $W(\cdot)$ y la fórmula
    \[ \sum_{k=1}^n k^2 = \frac{n(n+1)(2n+1)}{6}. \]
\end{tcolorbox} 

\textit{Solución}. Sean $a_1 = n, a_2 = n-1, \ldots, a_{n-1} = 2, a_n = 1$. Con estos coeficientes, construimos una combinación lineal de incrementos independientes, a saber,
\begin{align*}
    \begin{split}
    \sum_{i=1}^n a_i (W(i) - W(i-1)) &= a_1(W(1)-W(0)) + a_2(W(2) - W(1)) + \cdots \\ 
                                    & \quad \quad + a_{n-1}(W(n-1) - W(n-2)) + a_n(W(n) - W(n-1))
    \end{split}  \\
    \begin{split}
    &= (a_1-a_2)W(1) + (a_2-a_3)W(2) + \cdots + (a_{n-1}-a_n)W(n-1)\\ & \quad \quad +a_n W(n)
    \end{split}  \\
    &= W(1) + W(2) + \cdots + W(n-1) + W(n).
\end{align*}

Así pues, es posible expresar a $W(1) + \cdots + W(n)$ como una combinación lineal de incrementos independientes y, además, como tratamos con un proceso de Wiener, entonces
\[ W(i) - W(i-1) \sim N(0, \sigma^2), \]
tomando a $h=1$. Por lo tanto, como podemos expresar a $W(1) + \cdots + W(n)$ con una combinación lineal de componentes independientes y distribuidos normalmente, entonces $W(1) + \cdots + W(n)$ se distribuye normalmente, de manera que 
\[ W(1) + \cdots + W(n) = \sum_{i=1}^n a_i (W(i) - W(i-1)) \sim N(0, \sigma^2 \sum_{i=1}^{n}a_i^2), \]
pero 
\begin{align*}
    \sum_{i=1}^{n}a_i^2 &= \sum_{k=1}^n k^2 \\
    &= \frac{n(n+1)(2n+1)}{6}.
\end{align*}

Así pues, 
\[ W(1) + \cdots + W(n) \sim N \left(0, \sigma^2 \frac{n(n+1)(2n+1)}{6} \right). \]

% Pregunta 19
%------------------------------------------------------------
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!5!white, arc=0mm, boxrule=0pt]  
    \textbf{19.} Demuestre que cada uno de los siguientes PEs es de Wiener:
    \begin{itemize}
        \item[a)] $W_1(t) := a^{-1/2} W(at)$ para $t \geq 0$ (donde $a$ es una constante)
    \end{itemize}
\end{tcolorbox}

\textit{Solución}.

\textit{i}) P.D. que $W_1(0) = 0$ c.s.
\begin{align*}
    W_1(0) &:= a^{-1/2}W(a0) \\
    &= a^{-1/2}W(0) \\
    &= 0 \quad \textrm{c.s.}
\end{align*}
pues $W(0) = 0$ c.s. por ser proceso de Wiener.

\textit{ii})  P.D. que $W_1(\cdot)$ tiene incrementos independientes. Sean $0 \leq t_0 < t_1 < t_n$
\begin{align*}
    W_1(t_1) - W_1(t_0) &= a^{-1/2}W(at_1) - a^{-1/2}W(at_0) = a^{-1/2}[W(at_1)- W(at_0)] \\
    W_1(t_2) - W_1(t_1) &= a^{-1/2}W(at_2) - a^{-1/2}W(at_1) = a^{-1/2}[W(at_2)- W(at_1)] \\
    &\vdotswithin{=} \notag \\
    W_1(t_n) - W_1(t_{n-1}) &= a^{-1/2}W(at_n) - a^{-1/2}W(at_{n-1}) = a^{-1/2}[W(at_n)- W(at_{n-1})] \\
\end{align*}

Como $W(\cdot)$ es un proceso de Wiener, tiene incrementos independientes, por lo que 
\[ [W(at_1)- W(at_0)] \bot [W(at_2)- W(at_1)] \bot \cdots \bot [W(at_n)- W(at_{n-1})]  \]
y, por lo tanto, $W_1(\cdot)$ tiene incrementos independientes.

\textit{iii}) P.D. que $W_1(\cdot)$ tiene incrementos estacionarios con $W_1(t+h) - W_1(t) \sim N(0, \sigma^2 h)$.
\begin{align*}
    W_1(t+h) - W_1(t) &= a^{-1/2}W(a(t+h)) - a^{-1/2} W(at) \\
    &= a^{-1/2}[W(a(t+h)) - W(at)] \\
    &= a^{-1/2}[W(at+ah) - W(at)].
\end{align*}

Pero $[W(at+ah) - W(at)] \sim N(0, \sigma^2ah)$, por lo que
\begin{align*}
    W_1(t+h) - W_1(t) &\sim a^{-1/2} N(0, \sigma^2ah) \\
    &= N(a^{-1/2} 0, (a^{-1/2})^2 \sigma^2 ah) \\
    &= N(0, \sigma^2 h).
\end{align*}

Así, $W_1(\cdot)$ tiene incrementos estacionarios distribuidos normalmente y, por lo tanto, es un PE de Wiener.

\begin{tcolorbox}[colback=blue!5!white, colframe=blue!5!white, arc=0mm, boxrule=0pt]  
    \begin{itemize}
        \item[b)] $W_2(t) := tW(1/t)$ para $t > 0$ y $W_2(0):=0$
    \end{itemize}
\end{tcolorbox}

\textit{Solución}.

\textit{i}) $W_2(0) = 0$ por hipótesis.

\textit{ii}) P.D. que $W_2(\cdot)$ tiene incrementos independientes. Sean $0 \leq t_0 < t_1 < t_n$

\begin{align*}
    W_2(t_1) - W_2(t_0) &= t_1 W(1/t_1) - t_0W(1/t_0) \\
    W_2(t_2) - W_2(t_1) &= t_2 W(1/t_2) - t_1W(1/t_1) \\
    &\vdotswithin{=} \notag \\
    W_2(t_n) - W_2(t_{n-1}) &= t_n W(1/t_n) - t_{n-1}W(1/t_{n-1}).
\end{align*}

Como $W(\cdot)$ es un proceso de Wiener, tiene incrementos independientes, por lo que 
\[ [t_1 W(1/t_1)- t_0W(1/t_0)] \bot [t_2 W(1/t_2) - t_1W(1/t_1)] \bot \cdots \bot [t_n W(1/t_n) - t_{n-1}W(1/t_{n-1})]  \]
y, por lo tanto, $W_2(\cdot)$ tiene incrementos independientes.

\textit{iii}) P.D. que $W_2(\cdot)$ tiene incrementos estacionarios con $W_2(t+h) - W_2(t) \sim N(0, \sigma^2 h)$.
\begin{align*}
    W_2(t+h)-W_2(t) &= (t+h) W(1/(t+h)) - tW(1/t) \\
    &= -t [ W(1/t) - W(1/(t+h)) ] + hW(1/(t+h)).
\end{align*}

Como estamos tratando con una combinación lineal de elementos independientes y distribuidos normalmente, entonces la combinación es distribuida normalmente. Es decir, sea 
\[Y := -t [ W(1/t) - W(1/(t+h)) ] + hW(1/(t+h)). \]
Entonces $Y \sim N(\mu_y, \sigma_y^2)$, en donde
\begin{align*}
    \mu_y &= \E[Y] \\
    &= \E\{-t [ W(1/t) - W(1/(t+h)) ] + hW(1/(t+h))\} \\
    &= -t \E[W(1/t)] + t\E[W(1/(t+h))] + h\E[W(1/(t+h))] \\
    &= -t0 + t0 + h0 \\
    &= 0.
\end{align*}
Además,
\begin{align*}
    \sigma_y^2 &= \Var(Y) \\
    &= \Var(-t [ W(1/t) - W(1/(t+h)) ] + hW(1/(t+h))) \\
    &= \Var(-t [ W(1/t) - W(1/(t+h)) ] + h[W(1/(t+h))-W(0)]) \\
    &\overset{\mathrm{ind}}{=} t^2\Var[ W(1/t) - W(1/(t+h)) ] + h^2\Var[W(1/(t+h))-W(0)].
\end{align*}
Como $W(\cdot)$ es un proceso de Wiener, entonces $W(1/t) - W(1/(t+h)) \sim N(0, \sigma^2(1/t - 1/(t+h)))$, por lo que
\begin{align*}
    t^2\Var[ W(1/t) - W(1/(t+h)) ] &= t^2[\sigma^2(1/t - 1/(t+h))] \\
    &= t^2\sigma^2 \frac{t+h-t}{t(t+h)}  \\
    &= \frac{t \sigma^2 h}{t+h}.
\end{align*}
Similarmente, $W(1/(t+h))-W(0) \sim N(0, \sigma^2 (1/(t+h)))$, por lo que
\[ h^2\Var[W(1/(t+h))-W(0)] = \frac{h^2 \sigma^2}{t+h}. \]
Así pues, sumando ambos resultados,
\begin{align*}
    \sigma_y^2 &= \frac{t \sigma^2 h}{t+h} + \frac{h^2 \sigma^2}{t+h} \\
    &= \sigma^2h \left(\frac{t}{t+h} + \frac{h}{t+h} \right) \\
    &= \sigma^2h \left(\frac{t+h}{t+h} \right) \\
    &= \sigma^2h.
\end{align*}
Por lo tanto,  $W_2(t+h) - W_2(t) \sim N(0, \sigma^2 h)$ y, así, $W_2(\cdot)$ es un PE de Wiener.

% Pregunta 20
%------------------------------------------------------------
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!5!white, arc=0mm, boxrule=0pt]  
    \textbf{20.} Calcule $\E[X(t)]$ y $\Cov(X(s),X(t))$ para cada uno de los siguientes procesos
    \begin{itemize}
        \item[a)] $X(t):=W^2(t)$ para $t \geq 0$
    \end{itemize}
\end{tcolorbox}

\textit{Solución}.
\begin{align*}
    \E[X(t)] &= \E[W^2(t)] \\
    &= \Var(W(t)) + \E^2[W(t)] \\
    &= \sigma^2 t,
\end{align*}
pues $W(t) \sim N(0, \sigma^2t)$ por ser PE de Wiener.

Ahora bien, si $s<t$,
\begin{align*}
    K_X(s,t) &= \E[X(s)X(t)] - \E[X(s)]\E[X(t)] \\
    &= \E[X(s)X(t)] - \sigma^2s \sigma^2t \\
    &= \E[W^2(s)W^2(t)] - \sigma^4st \\
    &= \E[(W(s)W(t))^2] - \sigma^4st \\
    &= \E[(W(s)[W(t)-W(s)+W(s)])^2] - \sigma^4st \\
    &= \E[(W(s)(W(t)-W(s))+W^2(s))^2] - \sigma^4st \\
    &= \E[W^2(s)(W(t)-W(s))^2 + 2W^3(s)(W(t)-W(s)) + W^4(s)] - \sigma^4st \\
    &= \E[W^2(s)(W(t)-W(s))^2] + 2\E[W^3(s)(W(t)-W(s))] + \E[W^4(s)] - \sigma^4st \\
    &\overset{\mathrm{ind}}{=} \E[W^2(s)]\E[(W(t)-W(s))^2] + 2\E[W^3(s)]\E[W(t)-W(s)] + \E[W^4(s)] - \sigma^4st \\
    &= (\sigma^2 s)[\sigma^2(t-s)] + \E[W^4(s)] - \sigma^4st \\
    &= \sigma^4 s(t-s) + 3\sigma^4 s^2 - \sigma^4st \\
    &= \sigma^4 st - \sigma^4s^2 + 3\sigma^4 s^2 - \sigma^4st \\
    &= 2\sigma^4 s^2.
\end{align*}
Análogamente, si $t<s$, $K_X(s,t) = 2\sigma^4 t^2$. Por lo tanto, $K_X(s,t) = 2 \sigma^4 (\min(s,t))^2$.

\begin{tcolorbox}[colback=blue!5!white, colframe=blue!5!white, arc=0mm, boxrule=0pt]      
    \begin{itemize}
        \item[b)] $X(t):= e^{-at}W(e^{2at})$ para $t \geq 0$, donde $a \geq 0$ es constante.
    \end{itemize}
\end{tcolorbox}

\textit{Solución}.
\begin{align*}
    \E[X(t)] &= \E[e^{-at}W(e^{2at})] \\
    &= e^{-at} \E[W(e^{2at})] \\
    &= e^{-at} 0 \\
    &= 0.
\end{align*}
Ahora bien,
\begin{align*}
    K_X(s,t) &= \E[X(s)X(t)] - \E[X(s)]\E[X(t)] \\
    &= \E[X(s)X(t)] \\
    &= \E[e^{-as}W(e^{2as})e^{-at}W(e^{2at})] \\
    &= e^{-a(s+t)} \E[W(e^{2as})W(e^{2at})] \\
    &= e^{-a(s+t)} (K_W(e^{2as}, e^{2at}) + \E[W(e^{2as})]\E[W(e^{2at})]) \\
    &= e^{-a(s+t)} K_W(e^{2as}, e^{2at}) \\
    &= e^{-a(s+t)} \sigma^2 \min(e^{2as}, e^{2at}).
\end{align*}
\end{document}